ğŸï¸ Formula Q - OptimizaciÃ³n de Estrategia F1 con Q-Learning

Sistema de aprendizaje por refuerzo para determinar la estrategia Ã³ptima de carrera en FÃ³rmula 1 utilizando Q-Learning. El agente aprende quÃ© nivel de agresividad (conservadora, normal, agresiva) maximiza las probabilidades de ganar el campeonato segÃºn la posiciÃ³n de salida en parrilla.

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Status](https://img.shields.io/badge/Status-Active-success.svg)

DescripciÃ³n

Este proyecto simula un escenario real de FÃ³rmula 1 donde Lando Norris compite contra Max Verstappen y Oscar Piastri por el campeonato. El agente de Q-Learning aprende la polÃ­tica Ã³ptima considerando:

- **Estados**: PosiciÃ³n de salida en parrilla (Pole, P2-P3, P4-P6)
- **Acciones**: Estrategia de carrera (Conservadora, Normal, Agresiva)
- **Objetivo**: Maximizar probabilidad de ganar el campeonato
- **Trade-off**: Velocidad vs. riesgo de DNF (Did Not Finish)

CaracterÃ­sticas

- ImplementaciÃ³n completa de Q-Learning con epsilon-greedy
- Ambiente personalizado con distribuciones probabilÃ­sticas realistas
- SimulaciÃ³n visual interactiva con PyGame siguiendo trayectoria del circuito
- VisualizaciÃ³n de resultados (rewards, Q-table, polÃ­tica Ã³ptima)
- AnÃ¡lisis automÃ¡tico de estrategia aprendida
- Logging detallado del proceso de entrenamiento

InstalaciÃ³n

Requisitos
- Python 3.8 o superior
- pip

Pasos

1. **Clonar el repositorio**
```bash
git clone https://github.com/TaniaAraque/Proyecto-3-IA-F1-ML.git
cd Proyecto-3-IA-F1-ML
```

2. **Instalar dependencias**
```bash
pip install -r requirements.txt
```

Uso

### Entrenamiento y SimulaciÃ³n

Para entrenar el agente y ejecutar la simulaciÃ³n visual:

```bash
python main.py
```

Esto ejecutarÃ¡:
1. Entrenamiento con 10,000 episodios
2. GeneraciÃ³n de grÃ¡ficas en `/results`
3. AnÃ¡lisis de estrategia Ã³ptima (en consola)
4. SimulaciÃ³n visual animada (opcional)

Solo Entrenamiento (Modo RÃ¡pido)

Si deseas entrenar sin la simulaciÃ³n visual, comenta las lÃ­neas en `main.py`:

```python
# sim = F1RaceSim(env, Q, episodes=150)
# sim.run()
```
Herramienta de Mapeo de Circuito

Para crear tus propios waypoints del circuito:

```bash
python utils/track_mapper.py
```

Haz clic en la imagen para marcar puntos siguiendo la trayectoria.

Resultados

El sistema genera:

### 1. **Q-Table Final**
Matriz 3Ã—3 con valores Q aprendidos para cada combinaciÃ³n estado-acciÃ³n

### 2. **Estrategia Ã“ptima**
```
ESTRATEGIA Ã“PTIMA APRENDIDA:
============================================================
ğŸ¥‡ Desde Pole Position:
   â†’ Estrategia: Normal
   â†’ Probabilidad de victoria: 52.34%

ğŸ¥ˆ Desde P2-P3:
   â†’ Estrategia: Agresiva
   â†’ Probabilidad de victoria: 48.67%

ğŸ¥‰ Desde P4-P6:
   â†’ Estrategia: Normal
   â†’ Probabilidad de victoria: 39.12%
```

### 3. **Visualizaciones**
- `results/reward_curve.png` - Convergencia del aprendizaje
- `results/Qtable_heatmap.png` - Heatmap de valores Q
- `results/policy.png` - Mejor acciÃ³n por estado

## Estructura del Proyecto

```
F1-Q-Learning/
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ q_learning.py          # Algoritmo Q-Learning
â”œâ”€â”€ f1_env/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ environment.py         # Ambiente F1 personalizado
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ visualization.py       # GeneraciÃ³n de grÃ¡ficas
â”‚   â”œâ”€â”€ sim_pygame_race.py     # SimulaciÃ³n visual
â”‚   â””â”€â”€ track_mapper.py        # Herramienta de mapeo
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ track.png              # Imagen del circuito
â”‚   â”œâ”€â”€ car_norris.png         # Sprite Norris
â”‚   â””â”€â”€ car_rival.png          # Sprite rivales
â”œâ”€â”€ results/                   # GrÃ¡ficas y datos generados
â”œâ”€â”€ main.py                    # Script principal
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Fundamentos TeÃ³ricos

### Algoritmo Q-Learning

ActualizaciÃ³n de valores Q:

```
Q(s,a) â† Q(s,a) + Î±[r + Î³Â·max Q(s',a') - Q(s,a)]
```

Donde:
- `Î±` (alpha) = 0.1: Tasa de aprendizaje
- `Î³` (gamma) = 0.95: Factor de descuento
- `Îµ` (epsilon): 1.0 â†’ 0.05: ExploraciÃ³n vs explotaciÃ³n

### Distribuciones de Probabilidad

El ambiente modela 9 distribuciones diferentes segÃºn estado Ã— acciÃ³n, capturando:
- **Trade-off velocidad-riesgo**: Estrategias agresivas tienen mayor probabilidad de victoria pero tambiÃ©n de DNF
- **Impacto de posiciÃ³n inicial**: Desde pole es mÃ¡s fÃ¡cil ganar que desde atrÃ¡s
- **Realismo**: Basado en patrones observados en F1


## Autor

**Tania Julieth Araque DueÃ±as**
- GitHub: [@TaniaAraque](https://github.com/TaniaAraque)
- Proyecto: IntroducciÃ³n a Inteligencia Artificial

## Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver `LICENSE` para mÃ¡s detalles.


- Inspirado en la temporada 2024 de FÃ³rmula 1
- Implementado como proyecto educativo de Aprendizaje por Refuerzo
- Herramientas: Python, NumPy, Matplotlib, PyGame

---

â­ Si este proyecto te resultÃ³ Ãºtil, considera darle una estrella!
